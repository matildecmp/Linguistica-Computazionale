{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4to76Yc04omn"
   },
   "source": [
    "**PROGRAMMA 1**\n",
    "\n",
    "Dopo aver creato due corpora in lingua inglese, contenenti testi estratti rispettivamente da articoli di argomento scientifico tratti dal sito di *National* *Geographic* (\"natgeo.txt\") e da romanzi di fantascienza (\"scifi.txt\"), su tali corpora sono state eseguite le seguenti operazioni:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0gad9Kc4V9H",
    "outputId": "87179ecc-4b64-4462-f00e-a5b5df606ab8"
   },
   "outputs": [],
   "source": [
    "# MODULI NECESSARI:\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5nNLaOZiBboz"
   },
   "outputs": [],
   "source": [
    "# FUNZIONE CHE LEGGE E RESTITUISCE IL TESTO CONTENUTO IN CIASCUN FILE:\n",
    "\n",
    "def leggi_file(path_file):\n",
    "    with open (path_file, 'r') as file:\n",
    "      contenuto = file.read() # stringa con l'intero contenuto del file\n",
    "    return contenuto\n",
    "\n",
    "# VARIABILI PER I PATH DEI FILE, I RAW TEXT, LE LISTE DI FRASI E LE LISTE DI TOKEN DEI 2 CORPORA:\n",
    "\n",
    "# path dei file:\n",
    "f1 = \"./natgeo.txt\" \n",
    "f2 = \"./scifi.txt\" \n",
    "# raw text contenuti in ciascun file:\n",
    "contenuto_f1 = leggi_file(f1)\n",
    "contenuto_f2 = leggi_file(f2)\n",
    "# liste di frasi del testo (una per ciascun file):\n",
    "frasi_1 = nltk.tokenize.sent_tokenize(contenuto_f1)\n",
    "frasi_2 = nltk.tokenize.sent_tokenize(contenuto_f2)\n",
    "# liste di token del testo (una per ciascun file):\n",
    "tokens_1 = nltk.tokenize.word_tokenize(contenuto_f1)\n",
    "tokens_2 = nltk.tokenize.word_tokenize(contenuto_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_Kfw3Nu7E_H",
    "outputId": "0f56aa31-b965-4410-935e-668cc9b3cc34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di frasi del corpus 1:  291\n",
      "Numero di frasi del corpus 2:  730\n",
      "\n",
      "Numero di token/Lunghezza del corpus 1:  6889\n",
      "Numero di token/Lunghezza del corpus 2:  11451\n"
     ]
    }
   ],
   "source": [
    "# 1.1. FUNZIONE CHE RESTITUISCE IL NUMERO DI FRASI E QUELLO DI TOKEN DI CIASCUN CORPUS:\n",
    "\n",
    "def numero_frasi_e_token(testo):\n",
    "  frasi = nltk.tokenize.sent_tokenize(testo)\n",
    "  numero_frasi = len(frasi)\n",
    "  tokens = nltk.tokenize.word_tokenize(testo)\n",
    "  lunghezza_corpus = len(tokens)\n",
    "  return numero_frasi, lunghezza_corpus\n",
    "\n",
    "numero_frasi_1, lunghezza_corpus_1 = numero_frasi_e_token(contenuto_f1)\n",
    "numero_frasi_2, lunghezza_corpus_2 = numero_frasi_e_token(contenuto_f2)\n",
    "print(\"Numero di frasi del corpus 1: \", numero_frasi_1)\n",
    "print(\"Numero di frasi del corpus 2: \", numero_frasi_2)\n",
    "print()\n",
    "print(\"Numero di token/Lunghezza del corpus 1: \", lunghezza_corpus_1)\n",
    "print(\"Numero di token/Lunghezza del corpus 2: \", lunghezza_corpus_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Zl1UNYwMmu5",
    "outputId": "347706d5-fc16-431b-db13-42bf160e283a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lunghezza media in token delle frasi del corpus 1: 23.674\n",
      "Lunghezza media in token delle frasi del corpus 2: 15.686\n",
      "\n",
      "Lunghezza media in caratteri dei token del corpus 1: 4.836\n",
      "Lunghezza media in caratteri dei token del corpus 2: 4.340\n"
     ]
    }
   ],
   "source": [
    "# 1.2. FUNZIONI CHE RESTITUISCONO LA LUNGHEZZA MEDIA DELLE FRASI IN TOKEN E LA LUNGHEZZA MEDIA DEI TOKEN (ESCLUSA LA PUNTEGGIATURA) IN CARATTERI:\n",
    " \n",
    "def lunghezza_media_frasi(tokens, frasi):\n",
    "  lung_media = len(tokens)/len(frasi) # lunghezza media delle frasi = rapporto tra il numero totale di token nel corpus e il numero totale delle frasi\n",
    "  return lung_media\n",
    "    \n",
    "def lunghezza_media_tokens(tokens):\n",
    "  conta_punteggiatura = 0 # contatore per i segni di punteggiatura\n",
    "  conta_caratteri = 0 # contatore per i caratteri delle parole\n",
    "  for e in tokens: # per ogni elemento (parola o punteggiatura) nella lista dei token del corpus\n",
    "    if e in [\",\", \";\", \":\", \".\", \"?\", \"!\", \"-\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\"]: # se è un segno di punteggiatura\n",
    "      conta_punteggiatura += 1 # aggiorno il contatore dei segni di punteggiatura\n",
    "    else:\n",
    "      conta_caratteri += len(e) # calcolo la lunghezza della parola (= lista di caratteri)\n",
    "  lung_media_token = conta_caratteri / (len(tokens) - conta_punteggiatura) # lunghezza media dei token = rapporto tra il totale dei caratteri nel corpus e la lunghezza del corpus escluso il totale di segni di punteggiatura\n",
    "  return lung_media_token\n",
    "\n",
    "print(f\"Lunghezza media in token delle frasi del corpus 1: {lunghezza_media_frasi(tokens_1, frasi_1):.3f}\")\n",
    "print(f\"Lunghezza media in token delle frasi del corpus 2: {lunghezza_media_frasi(tokens_2, frasi_2):.3f}\")\n",
    "print()\n",
    "print(f\"Lunghezza media in caratteri dei token del corpus 1: {lunghezza_media_tokens(tokens_1):.3f}\")\n",
    "print(f\"Lunghezza media in caratteri dei token del corpus 2: {lunghezza_media_tokens(tokens_2):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viUIkOVZ7MYx",
    "outputId": "863dc3e6-2013-451f-8e66-1d5e15231e2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS 1:\n",
      "Numero di hapax nei primi 500 token del corpus: 77\n",
      "Numero di hapax nei primi 1000 token del corpus: 154\n",
      "Numero di hapax nei primi 3000 token del corpus: 491\n",
      "Numero di hapax nei primi 6889 token del corpus: 1245\n",
      "\n",
      "CORPUS 2:\n",
      "Numero di hapax nei primi 500 token del corpus: 123\n",
      "Numero di hapax nei primi 1000 token del corpus: 215\n",
      "Numero di hapax nei primi 3000 token del corpus: 586\n",
      "Numero di hapax nei primi 11451 token del corpus: 1734\n"
     ]
    }
   ],
   "source": [
    "# 1.3. FUNZIONE CHE RESTITUISCE IL NUMERO DI HAPAX TRA I PRIMI 500, 1000, 3000 TOKEN, E NELL'INTERO CORPUS:\n",
    "\n",
    "def numero_hapax_per_porzione(tokens):\n",
    "  valori_slicing = [500, 1000, 3000, len(tokens)] # lista con le porzioni in cui suddividere di volta in volta il corpus\n",
    "  for valore in valori_slicing:\n",
    "    conta_hapax = 0 # ad ogni iterazione (= per ogni porzione considerata), il conto degli hapax si azzera\n",
    "    for token in tokens[0:valore]: # per ogni token nella lista di token considerata dal primo token del corpus a uno degli indici nella lista \"valori_slicing\"\n",
    "      if tokens.count(token) == 1: # se il token ricorre 1 sola volta nel corpus (= è un hapax)\n",
    "        conta_hapax += 1 # aggiorno il conto degli hapax\n",
    "    print(f\"Numero di hapax nei primi {valore} token del corpus: {conta_hapax}\")\n",
    "\n",
    "print(\"CORPUS 1:\")\n",
    "numero_hapax_per_porzione_1 = numero_hapax_per_porzione(tokens_1)\n",
    "print()\n",
    "print(\"CORPUS 2:\")\n",
    "numero_hapax_per_porzione_2 = numero_hapax_per_porzione(tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYlacLRm-7fq",
    "outputId": "18b995d6-52df-4335-9fd0-974f410b08a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS 1:\n",
      "Grandezza del vocabolario per i primi 200 token del corpus: 126\n",
      "TTR/Indice di ricchezza lessicale per i primi 200 token del corpus: 0.630\n",
      "Grandezza del vocabolario per i primi 400 token del corpus: 224\n",
      "TTR/Indice di ricchezza lessicale per i primi 400 token del corpus: 0.560\n",
      "Grandezza del vocabolario per i primi 600 token del corpus: 285\n",
      "TTR/Indice di ricchezza lessicale per i primi 600 token del corpus: 0.475\n",
      "Grandezza del vocabolario per i primi 800 token del corpus: 347\n",
      "TTR/Indice di ricchezza lessicale per i primi 800 token del corpus: 0.434\n",
      "Grandezza del vocabolario per i primi 1000 token del corpus: 410\n",
      "TTR/Indice di ricchezza lessicale per i primi 1000 token del corpus: 0.410\n",
      "Grandezza del vocabolario per i primi 1200 token del corpus: 465\n",
      "TTR/Indice di ricchezza lessicale per i primi 1200 token del corpus: 0.388\n",
      "Grandezza del vocabolario per i primi 1400 token del corpus: 515\n",
      "TTR/Indice di ricchezza lessicale per i primi 1400 token del corpus: 0.368\n",
      "Grandezza del vocabolario per i primi 1600 token del corpus: 575\n",
      "TTR/Indice di ricchezza lessicale per i primi 1600 token del corpus: 0.359\n",
      "Grandezza del vocabolario per i primi 1800 token del corpus: 643\n",
      "TTR/Indice di ricchezza lessicale per i primi 1800 token del corpus: 0.357\n",
      "Grandezza del vocabolario per i primi 2000 token del corpus: 716\n",
      "TTR/Indice di ricchezza lessicale per i primi 2000 token del corpus: 0.358\n",
      "Grandezza del vocabolario per i primi 2200 token del corpus: 788\n",
      "TTR/Indice di ricchezza lessicale per i primi 2200 token del corpus: 0.358\n",
      "Grandezza del vocabolario per i primi 2400 token del corpus: 848\n",
      "TTR/Indice di ricchezza lessicale per i primi 2400 token del corpus: 0.353\n",
      "Grandezza del vocabolario per i primi 2600 token del corpus: 895\n",
      "TTR/Indice di ricchezza lessicale per i primi 2600 token del corpus: 0.344\n",
      "Grandezza del vocabolario per i primi 2800 token del corpus: 935\n",
      "TTR/Indice di ricchezza lessicale per i primi 2800 token del corpus: 0.334\n",
      "Grandezza del vocabolario per i primi 3000 token del corpus: 981\n",
      "TTR/Indice di ricchezza lessicale per i primi 3000 token del corpus: 0.327\n",
      "Grandezza del vocabolario per i primi 3200 token del corpus: 1041\n",
      "TTR/Indice di ricchezza lessicale per i primi 3200 token del corpus: 0.325\n",
      "Grandezza del vocabolario per i primi 3400 token del corpus: 1108\n",
      "TTR/Indice di ricchezza lessicale per i primi 3400 token del corpus: 0.326\n",
      "Grandezza del vocabolario per i primi 3600 token del corpus: 1165\n",
      "TTR/Indice di ricchezza lessicale per i primi 3600 token del corpus: 0.324\n",
      "Grandezza del vocabolario per i primi 3800 token del corpus: 1225\n",
      "TTR/Indice di ricchezza lessicale per i primi 3800 token del corpus: 0.322\n",
      "Grandezza del vocabolario per i primi 4000 token del corpus: 1261\n",
      "TTR/Indice di ricchezza lessicale per i primi 4000 token del corpus: 0.315\n",
      "Grandezza del vocabolario per i primi 4200 token del corpus: 1300\n",
      "TTR/Indice di ricchezza lessicale per i primi 4200 token del corpus: 0.310\n",
      "Grandezza del vocabolario per i primi 4400 token del corpus: 1338\n",
      "TTR/Indice di ricchezza lessicale per i primi 4400 token del corpus: 0.304\n",
      "Grandezza del vocabolario per i primi 4600 token del corpus: 1386\n",
      "TTR/Indice di ricchezza lessicale per i primi 4600 token del corpus: 0.301\n",
      "Grandezza del vocabolario per i primi 4800 token del corpus: 1446\n",
      "TTR/Indice di ricchezza lessicale per i primi 4800 token del corpus: 0.301\n",
      "Grandezza del vocabolario per i primi 5000 token del corpus: 1511\n",
      "TTR/Indice di ricchezza lessicale per i primi 5000 token del corpus: 0.302\n",
      "Grandezza del vocabolario per i primi 5200 token del corpus: 1556\n",
      "TTR/Indice di ricchezza lessicale per i primi 5200 token del corpus: 0.299\n",
      "Grandezza del vocabolario per i primi 5400 token del corpus: 1621\n",
      "TTR/Indice di ricchezza lessicale per i primi 5400 token del corpus: 0.300\n",
      "Grandezza del vocabolario per i primi 5600 token del corpus: 1679\n",
      "TTR/Indice di ricchezza lessicale per i primi 5600 token del corpus: 0.300\n",
      "Grandezza del vocabolario per i primi 5800 token del corpus: 1748\n",
      "TTR/Indice di ricchezza lessicale per i primi 5800 token del corpus: 0.301\n",
      "Grandezza del vocabolario per i primi 6000 token del corpus: 1796\n",
      "TTR/Indice di ricchezza lessicale per i primi 6000 token del corpus: 0.299\n",
      "Grandezza del vocabolario per i primi 6200 token del corpus: 1837\n",
      "TTR/Indice di ricchezza lessicale per i primi 6200 token del corpus: 0.296\n",
      "Grandezza del vocabolario per i primi 6400 token del corpus: 1886\n",
      "TTR/Indice di ricchezza lessicale per i primi 6400 token del corpus: 0.295\n",
      "Grandezza del vocabolario per i primi 6600 token del corpus: 1920\n",
      "TTR/Indice di ricchezza lessicale per i primi 6600 token del corpus: 0.291\n",
      "Grandezza del vocabolario per i primi 6800 token del corpus: 1957\n",
      "TTR/Indice di ricchezza lessicale per i primi 6800 token del corpus: 0.288\n",
      "\n",
      "CORPUS 2:\n",
      "Grandezza del vocabolario per i primi 200 token del corpus: 121\n",
      "TTR/Indice di ricchezza lessicale per i primi 200 token del corpus: 0.605\n",
      "Grandezza del vocabolario per i primi 400 token del corpus: 223\n",
      "TTR/Indice di ricchezza lessicale per i primi 400 token del corpus: 0.557\n",
      "Grandezza del vocabolario per i primi 600 token del corpus: 303\n",
      "TTR/Indice di ricchezza lessicale per i primi 600 token del corpus: 0.505\n",
      "Grandezza del vocabolario per i primi 800 token del corpus: 377\n",
      "TTR/Indice di ricchezza lessicale per i primi 800 token del corpus: 0.471\n",
      "Grandezza del vocabolario per i primi 1000 token del corpus: 456\n",
      "TTR/Indice di ricchezza lessicale per i primi 1000 token del corpus: 0.456\n",
      "Grandezza del vocabolario per i primi 1200 token del corpus: 521\n",
      "TTR/Indice di ricchezza lessicale per i primi 1200 token del corpus: 0.434\n",
      "Grandezza del vocabolario per i primi 1400 token del corpus: 593\n",
      "TTR/Indice di ricchezza lessicale per i primi 1400 token del corpus: 0.424\n",
      "Grandezza del vocabolario per i primi 1600 token del corpus: 664\n",
      "TTR/Indice di ricchezza lessicale per i primi 1600 token del corpus: 0.415\n",
      "Grandezza del vocabolario per i primi 1800 token del corpus: 733\n",
      "TTR/Indice di ricchezza lessicale per i primi 1800 token del corpus: 0.407\n",
      "Grandezza del vocabolario per i primi 2000 token del corpus: 791\n",
      "TTR/Indice di ricchezza lessicale per i primi 2000 token del corpus: 0.396\n",
      "Grandezza del vocabolario per i primi 2200 token del corpus: 856\n",
      "TTR/Indice di ricchezza lessicale per i primi 2200 token del corpus: 0.389\n",
      "Grandezza del vocabolario per i primi 2400 token del corpus: 924\n",
      "TTR/Indice di ricchezza lessicale per i primi 2400 token del corpus: 0.385\n",
      "Grandezza del vocabolario per i primi 2600 token del corpus: 979\n",
      "TTR/Indice di ricchezza lessicale per i primi 2600 token del corpus: 0.377\n",
      "Grandezza del vocabolario per i primi 2800 token del corpus: 1048\n",
      "TTR/Indice di ricchezza lessicale per i primi 2800 token del corpus: 0.374\n",
      "Grandezza del vocabolario per i primi 3000 token del corpus: 1107\n",
      "TTR/Indice di ricchezza lessicale per i primi 3000 token del corpus: 0.369\n",
      "Grandezza del vocabolario per i primi 3200 token del corpus: 1167\n",
      "TTR/Indice di ricchezza lessicale per i primi 3200 token del corpus: 0.365\n",
      "Grandezza del vocabolario per i primi 3400 token del corpus: 1209\n",
      "TTR/Indice di ricchezza lessicale per i primi 3400 token del corpus: 0.356\n",
      "Grandezza del vocabolario per i primi 3600 token del corpus: 1273\n",
      "TTR/Indice di ricchezza lessicale per i primi 3600 token del corpus: 0.354\n",
      "Grandezza del vocabolario per i primi 3800 token del corpus: 1336\n",
      "TTR/Indice di ricchezza lessicale per i primi 3800 token del corpus: 0.352\n",
      "Grandezza del vocabolario per i primi 4000 token del corpus: 1396\n",
      "TTR/Indice di ricchezza lessicale per i primi 4000 token del corpus: 0.349\n",
      "Grandezza del vocabolario per i primi 4200 token del corpus: 1460\n",
      "TTR/Indice di ricchezza lessicale per i primi 4200 token del corpus: 0.348\n",
      "Grandezza del vocabolario per i primi 4400 token del corpus: 1502\n",
      "TTR/Indice di ricchezza lessicale per i primi 4400 token del corpus: 0.341\n",
      "Grandezza del vocabolario per i primi 4600 token del corpus: 1551\n",
      "TTR/Indice di ricchezza lessicale per i primi 4600 token del corpus: 0.337\n",
      "Grandezza del vocabolario per i primi 4800 token del corpus: 1584\n",
      "TTR/Indice di ricchezza lessicale per i primi 4800 token del corpus: 0.330\n",
      "Grandezza del vocabolario per i primi 5000 token del corpus: 1621\n",
      "TTR/Indice di ricchezza lessicale per i primi 5000 token del corpus: 0.324\n",
      "Grandezza del vocabolario per i primi 5200 token del corpus: 1649\n",
      "TTR/Indice di ricchezza lessicale per i primi 5200 token del corpus: 0.317\n",
      "Grandezza del vocabolario per i primi 5400 token del corpus: 1681\n",
      "TTR/Indice di ricchezza lessicale per i primi 5400 token del corpus: 0.311\n",
      "Grandezza del vocabolario per i primi 5600 token del corpus: 1711\n",
      "TTR/Indice di ricchezza lessicale per i primi 5600 token del corpus: 0.306\n",
      "Grandezza del vocabolario per i primi 5800 token del corpus: 1747\n",
      "TTR/Indice di ricchezza lessicale per i primi 5800 token del corpus: 0.301\n",
      "Grandezza del vocabolario per i primi 6000 token del corpus: 1773\n",
      "TTR/Indice di ricchezza lessicale per i primi 6000 token del corpus: 0.295\n",
      "Grandezza del vocabolario per i primi 6200 token del corpus: 1800\n",
      "TTR/Indice di ricchezza lessicale per i primi 6200 token del corpus: 0.290\n",
      "Grandezza del vocabolario per i primi 6400 token del corpus: 1853\n",
      "TTR/Indice di ricchezza lessicale per i primi 6400 token del corpus: 0.290\n",
      "Grandezza del vocabolario per i primi 6600 token del corpus: 1910\n",
      "TTR/Indice di ricchezza lessicale per i primi 6600 token del corpus: 0.289\n",
      "Grandezza del vocabolario per i primi 6800 token del corpus: 1951\n",
      "TTR/Indice di ricchezza lessicale per i primi 6800 token del corpus: 0.287\n",
      "Grandezza del vocabolario per i primi 7000 token del corpus: 1984\n",
      "TTR/Indice di ricchezza lessicale per i primi 7000 token del corpus: 0.283\n",
      "Grandezza del vocabolario per i primi 7200 token del corpus: 2025\n",
      "TTR/Indice di ricchezza lessicale per i primi 7200 token del corpus: 0.281\n",
      "Grandezza del vocabolario per i primi 7400 token del corpus: 2060\n",
      "TTR/Indice di ricchezza lessicale per i primi 7400 token del corpus: 0.278\n",
      "Grandezza del vocabolario per i primi 7600 token del corpus: 2101\n",
      "TTR/Indice di ricchezza lessicale per i primi 7600 token del corpus: 0.276\n",
      "Grandezza del vocabolario per i primi 7800 token del corpus: 2135\n",
      "TTR/Indice di ricchezza lessicale per i primi 7800 token del corpus: 0.274\n",
      "Grandezza del vocabolario per i primi 8000 token del corpus: 2161\n",
      "TTR/Indice di ricchezza lessicale per i primi 8000 token del corpus: 0.270\n",
      "Grandezza del vocabolario per i primi 8200 token del corpus: 2188\n",
      "TTR/Indice di ricchezza lessicale per i primi 8200 token del corpus: 0.267\n",
      "Grandezza del vocabolario per i primi 8400 token del corpus: 2219\n",
      "TTR/Indice di ricchezza lessicale per i primi 8400 token del corpus: 0.264\n",
      "Grandezza del vocabolario per i primi 8600 token del corpus: 2243\n",
      "TTR/Indice di ricchezza lessicale per i primi 8600 token del corpus: 0.261\n",
      "Grandezza del vocabolario per i primi 8800 token del corpus: 2274\n",
      "TTR/Indice di ricchezza lessicale per i primi 8800 token del corpus: 0.258\n",
      "Grandezza del vocabolario per i primi 9000 token del corpus: 2295\n",
      "TTR/Indice di ricchezza lessicale per i primi 9000 token del corpus: 0.255\n",
      "Grandezza del vocabolario per i primi 9200 token del corpus: 2339\n",
      "TTR/Indice di ricchezza lessicale per i primi 9200 token del corpus: 0.254\n",
      "Grandezza del vocabolario per i primi 9400 token del corpus: 2369\n",
      "TTR/Indice di ricchezza lessicale per i primi 9400 token del corpus: 0.252\n",
      "Grandezza del vocabolario per i primi 9600 token del corpus: 2407\n",
      "TTR/Indice di ricchezza lessicale per i primi 9600 token del corpus: 0.251\n",
      "Grandezza del vocabolario per i primi 9800 token del corpus: 2443\n",
      "TTR/Indice di ricchezza lessicale per i primi 9800 token del corpus: 0.249\n",
      "Grandezza del vocabolario per i primi 10000 token del corpus: 2485\n",
      "TTR/Indice di ricchezza lessicale per i primi 10000 token del corpus: 0.248\n",
      "Grandezza del vocabolario per i primi 10200 token del corpus: 2508\n",
      "TTR/Indice di ricchezza lessicale per i primi 10200 token del corpus: 0.246\n",
      "Grandezza del vocabolario per i primi 10400 token del corpus: 2544\n",
      "TTR/Indice di ricchezza lessicale per i primi 10400 token del corpus: 0.245\n",
      "Grandezza del vocabolario per i primi 10600 token del corpus: 2574\n",
      "TTR/Indice di ricchezza lessicale per i primi 10600 token del corpus: 0.243\n",
      "Grandezza del vocabolario per i primi 10800 token del corpus: 2597\n",
      "TTR/Indice di ricchezza lessicale per i primi 10800 token del corpus: 0.240\n",
      "Grandezza del vocabolario per i primi 11000 token del corpus: 2620\n",
      "TTR/Indice di ricchezza lessicale per i primi 11000 token del corpus: 0.238\n",
      "Grandezza del vocabolario per i primi 11200 token del corpus: 2647\n",
      "TTR/Indice di ricchezza lessicale per i primi 11200 token del corpus: 0.236\n",
      "Grandezza del vocabolario per i primi 11400 token del corpus: 2682\n",
      "TTR/Indice di ricchezza lessicale per i primi 11400 token del corpus: 0.235\n"
     ]
    }
   ],
   "source": [
    "# 1.4. FUNZIONE CHE RESTITUISCE LA DIMENSIONE DEL VOCABOLARIO E LA TTR CALCOLATA PER PORZIONI INCREMENTALI DI 200 TOKEN:\n",
    "\n",
    "def dimensione_vocabolario_e_ttr_200(tokens):\n",
    "  i = 200 # valore dell'incremento della porzione del corpus considerata ad ogni ciclo\n",
    "  while i < len(tokens): # finché il valore dell'incremento è minore della lunghezza del corpus\n",
    "    porzione = tokens[0:i] # considero la porzione del corpus che va dal primo token a quello di indice \"i\"\n",
    "    vocabolario_porzione = list(set(porzione)) # vocabolario (= numero di types) della porzione \n",
    "    ttr_porzione = len(vocabolario_porzione)/len(porzione) # TTR della porzione\n",
    "    print(f\"Grandezza del vocabolario per i primi {i} token del corpus: {len(vocabolario_porzione)}\")\n",
    "    print(f\"TTR/Indice di ricchezza lessicale per i primi {i} token del corpus: {ttr_porzione:.3f}\")\n",
    "    i += 200 # la lunghezza della porzione del corpus considerata aumenta di 200 token ad ogni iterazione \n",
    "\n",
    "print(\"CORPUS 1:\")\n",
    "dimensione_vocabolario_e_ttr_200(tokens_1)\n",
    "print()\n",
    "print(\"CORPUS 2:\")\n",
    "dimensione_vocabolario_e_ttr_200(tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nG-_81CK3UAp",
    "outputId": "e7a758e2-fccf-40c5-aab0-59de70b4db48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di lemmi distinti del corpus 1: 1734\n",
      "Numero di lemmi distinti del corpus 2: 2367\n"
     ]
    }
   ],
   "source": [
    "# 1.5. FUNZIONE CHE RESTITUISCE IL NUMERO DI LEMMI DISTINTI\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "def converti_pos_tagger(pos): # funzione che converte il PoS-tagging del Penn Treebank in quello di WordNet\n",
    "  if pos.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  if pos.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  if pos.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  if pos.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def numero_lemmi_distinti(tokens):\n",
    "  lemmatizer = WordNetLemmatizer() \n",
    "  lemmi = [] # lista per i lemmi\n",
    "  token_e_pos = nltk.tag.pos_tag(tokens) # lista per le coppie (token, PoS)\n",
    "  for token, pos in token_e_pos: # per ogni token e rispettiva PoS nella lista\n",
    "    pos_wordnet = converti_pos_tagger(pos) # converto la PoS secondo lo standard di WordNet\n",
    "    lemma = lemmatizer.lemmatize(token, pos_wordnet) # e lemmatizzo il token (fornendo al lemmatizzatore anche la PoS corretta, c'è minor margine di errore)\n",
    "    lemmi.append(lemma) # inserisco il lemma ottenuto nella lista\n",
    "    vocabolario_lemmi = list(set(lemmi)) # estraggo il vocabolario dei lemmi (= lista senza occorrenze ripetute)\n",
    "  return len(vocabolario_lemmi)\n",
    "\n",
    "print(\"Numero di lemmi distinti del corpus 1:\", numero_lemmi_distinti(tokens_1))\n",
    "print(\"Numero di lemmi distinti del corpus 2:\", numero_lemmi_distinti(tokens_2))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
