{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_zhb_2bp__K"
   },
   "source": [
    "**PROGRAMMA 2.1**\n",
    "\n",
    "Dopo aver creato un corpus in lingua inglese, contenente testi estratti da articoli di argomento scientifico tratti dal sito di *National* *Geographic* (\"natgeo.txt\"), il programma estrae le seguenti informazioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqsPdLg50D2R",
    "outputId": "416ed85f-19c5-44f3-d9bb-b3250e076dee"
   },
   "outputs": [],
   "source": [
    "# MODULI NECESSARI:\n",
    "\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tfml0iio0gmI"
   },
   "outputs": [],
   "source": [
    "# FUNZIONE CHE LEGGE E RESTITUISCE IL TESTO CONTENUTO NEL FILE:\n",
    "\n",
    "def leggi_file(path_file):\n",
    "    with open (path_file, 'r') as file:\n",
    "      contenuto = file.read() # stringa con l'intero contenuto del file\n",
    "    return contenuto\n",
    "\n",
    "# FUNZIONE CHE OPERA IL POS-TAGGING SUI TOKEN DEL TESTO:\n",
    "\n",
    "def pos_tagger(tokens):\n",
    "  pos_tagging = nltk.tag.pos_tag(tokens)\n",
    "  return pos_tagging\n",
    "\n",
    "# VARIABILI PER IL PATH DEL FILE, IL RAW TEXT, LA LISTA DI TOKEN, LA LISTA DI FRASI E LA LISTA DI COPPIE TOKEN-POS DEL TESTO:\n",
    "\n",
    "# path del file:\n",
    "f1 = \"./natgeo.txt\" \n",
    "# raw text:\n",
    "contenuto_f1 = leggi_file(f1)\n",
    "# lista di frasi del testo:\n",
    "frasi_1 = nltk.tokenize.sent_tokenize(contenuto_f1)\n",
    "# lista di token del testo:\n",
    "tokens_1 = nltk.tokenize.word_tokenize(contenuto_f1)\n",
    "# lista token-PoS del testo:\n",
    "token_pos_1 = pos_tagger(tokens_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I__i8CjWICrw",
    "outputId": "89422d23-6db8-4f17-caf8-2f4c23b895d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 PoS più frequenti nel corpus: [('NN', 977), ('IN', 753), ('DT', 641), ('JJ', 537), ('NNS', 456), ('NNP', 362), (',', 342), ('.', 291), ('RB', 276), ('VBZ', 215)]\n",
      "\n",
      "10 bigrammi di PoS più frequenti nel corpus: [(('DT', 'NN'), 306), (('IN', 'DT'), 278), (('JJ', 'NN'), 240), (('NN', 'IN'), 199), (('DT', 'JJ'), 173), (('NN', ','), 141), (('NN', '.'), 131), (('JJ', 'NNS'), 131), (('NNS', 'IN'), 124), (('NN', 'NN'), 108)]\n",
      "\n",
      "10 trigrammi di PoS più frequenti nel corpus: [(('IN', 'DT', 'NN'), 125), (('DT', 'JJ', 'NN'), 114), (('IN', 'DT', 'JJ'), 76), (('NN', 'IN', 'DT'), 75), (('DT', 'NN', 'IN'), 71), (('JJ', 'NN', 'IN'), 49), (('DT', 'NN', '.'), 40), (('NNS', 'IN', 'DT'), 40), (('JJ', 'NN', '.'), 40), (('JJ', 'NNS', 'IN'), 39)]\n",
      "\n",
      "20 sostantivi più frequenti nel corpus: [('star', 29), ('system', 22), ('planets', 21), ('Yellowstone', 20), ('NASA', 19), ('planet', 18), ('scientists', 15), ('world', 14), ('sun', 13), ('team', 13), ('National', 13), ('climate', 13), ('biofluorescence', 12), ('years', 11), ('dwarf', 11), ('Earth', 11), ('Park', 11), ('moon', 11), ('mission', 11), ('astronauts', 11)]\n",
      "\n",
      "20 avverbi più frequenti nel corpus: [('not', 18), ('just', 15), (\"n't\", 14), ('even', 10), ('away', 9), ('also', 8), ('as', 8), ('then', 7), ('very', 6), ('so', 6), ('well', 6), ('really', 5), ('only', 5), ('once', 4), ('now', 4), ('far', 4), ('roughly', 4), ('So', 4), ('back', 4), ('newly', 3)]\n",
      "\n",
      "20 aggettivi più frequenti nel corpus: [('white', 16), ('new', 9), ('different', 9), ('light', 9), ('giant', 8), ('own', 8), ('other', 8), ('solar', 7), ('red', 7), ('recent', 7), ('likely', 6), ('first', 6), ('such', 6), ('real', 6), ('fluorescent', 6), ('planetary', 5), ('many', 5), ('north', 5), ('lunar', 5), ('marine', 5)]\n"
     ]
    }
   ],
   "source": [
    "# 2.1. FUNZIONI CHE RESTITUISCONO LA SEQUENZA, ORDINATA PER FREQUENZA DECRESCENTE E CON RELATIVA FREQUENZA, DI:\n",
    "\n",
    "# A) 10 POS, BIGRAMMI DI POS E TRIGRAMMI DI POS PIU' FREQUENTI:\n",
    "\n",
    "def dieci_elementi_più_freq(tokens, n):\n",
    "  lista_token_pos = pos_tagger(tokens) # lista per le coppie (token, PoS), ottenuta invocando la funzione nella cella precedente\n",
    "  solo_pos = [] # lista per le sole PoS\n",
    "  for token, pos in lista_token_pos: # per ogni coppia (token, PoS) in \"lista_token_pos\"\n",
    "    solo_pos.append(pos) # estraggo solo la PoS e la inserisco nella lista \"solo_pos\"\n",
    "  if n != 1: # se considero gli n-grammi di PoS\n",
    "    ngrammi = list(nltk.ngrams(solo_pos, n)) # creo una lista di n-grammi di PoS\n",
    "    frequenze = nltk.FreqDist(ngrammi) # calcolo le frequenze (già ordinate per valori decrescenti, grazie alla funzione FreqDist()) degli n-grammi nella lista\n",
    "    return frequenze.most_common(10) # e restituisco i 10 n-grammi di PoS più frequenti\n",
    "  else: # se invece considero le PoS dei singoli token\n",
    "    frequenze = nltk.FreqDist(solo_pos) # calcolo le frequenze delle PoS nella lista\n",
    "    return frequenze.most_common(10) # e restituisco le 10 PoS più frequenti\n",
    "\n",
    "print(f\"10 PoS più frequenti nel corpus: {dieci_elementi_più_freq(tokens_1, 1)}\")\n",
    "print()\n",
    "print(f\"10 bigrammi di PoS più frequenti nel corpus: {dieci_elementi_più_freq(tokens_1, 2)}\")\n",
    "print()\n",
    "print(f\"10 trigrammi di PoS più frequenti nel corpus: {dieci_elementi_più_freq(tokens_1, 3)}\")\n",
    "print()\n",
    "\n",
    "# B) 20 SOSTANTIVI, AVVERBI E AGGETTIVI PIU' FREQUENTI:\n",
    "\n",
    "def venti_elementi_più_freq(lista_token_pos, lista_tag): # funzione che prende in input la lista di coppie (token, PoS) e la lista che contiene uno o più tag associati ad una data PoS (es: per la PoS dei nomi, si hanno i tag NN, NNP, NNPS, ecc.)\n",
    "  lista_token_tag = [] # lista di token identificati da uno dei possibili tag della PoS (= \"lista_tag\") in input \n",
    "  for token, pos in lista_token_pos: # per ogni coppia (token, PoS) in \"lista_token_pos\"\n",
    "    for tag in lista_tag: # per ogni tag in \"lista_tag\"\n",
    "      if pos == tag: # se il PoS-tag del token coincide con un tag della lista in input\n",
    "        lista_token_tag.append(token) # inserisco il token in \"lista_token_tag\"\n",
    "  frequenze = nltk.FreqDist(lista_token_tag) # calcolo le frequenze dei token nella lista\n",
    "  return frequenze.most_common(20) # restituisco i 20 token più frequenti\n",
    "\n",
    "print(f\"20 sostantivi più frequenti nel corpus: {venti_elementi_più_freq(token_pos_1, ['NN', 'NNP', 'NNPS', 'NNS'])}\")\n",
    "print()\n",
    "print(f\"20 avverbi più frequenti nel corpus: {venti_elementi_più_freq(token_pos_1, ['RB'])}\")\n",
    "print()\n",
    "print(f\"20 aggettivi più frequenti nel corpus: {venti_elementi_più_freq(token_pos_1, ['JJ'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tascy0V7PeC5",
    "outputId": "f5abb755-fe14-4225-c6dc-2dc5eceae22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 bigrammi Aggettivo-Sostantivo più frequenti nel corpus: [((('white', 'JJ'), ('dwarf', 'NN')), 11), ((('solar', 'JJ'), ('system', 'NN')), 6), ((('white', 'JJ'), ('dwarfs', 'NN')), 5), ((('north', 'JJ'), ('entrance', 'NN')), 4), ((('giant', 'JJ'), ('planets', 'NNS')), 3), ((('stellar', 'JJ'), ('corpse', 'NN')), 2), ((('red', 'JJ'), ('giant', 'NN')), 2), ((('giant', 'JJ'), ('star', 'NN')), 2), ((('same', 'JJ'), ('place', 'NN')), 2), ((('annual', 'JJ'), ('precipitation', 'NN')), 2), ((('recent', 'JJ'), ('flood', 'NN')), 2), ((('cubic', 'JJ'), ('feet', 'NNS')), 2), ((('behavioral', 'JJ'), ('manipulations', 'NNS')), 2), ((('different', 'JJ'), ('planet', 'NN')), 2), ((('yellow', 'JJ'), ('mask', 'NN')), 2), ((('fluorescent', 'JJ'), ('diving', 'NN')), 2), ((('burned-out', 'JJ'), ('star', 'NN')), 1), ((('Jupiter-size', 'JJ'), ('world', 'NN')), 1), ((('avoided', 'JJ'), ('destruction', 'NN')), 1), ((('gassy', 'JJ'), ('world', 'NN')), 1)]\n",
      "\n",
      "20 bigrammi Aggettivo-Sostantivo nel corpus con massima probabilità condizionata e relativo valore di probabilità: [((('fungus-caterpillar', 'JJ'), ('husk', 'NN')), 1.0), ((('nuclear', 'JJ'), ('furnaces', 'NNS')), 1.0), ((('robust', 'JJ'), ('systems', 'NNS')), 1.0), ((('low-resolution', 'JJ'), ('maps', 'NNS')), 1.0), ((('moon-like', 'JJ'), ('landscape', 'NN')), 1.0), ((('scuba', 'JJ'), ('blog', 'NN')), 1.0), ((('inner', 'JJ'), ('part', 'NN')), 1.0), ((('evolutionary', 'JJ'), ('transformations', 'NNS')), 1.0), ((('night-dive', 'JJ'), ('specialty', 'NN')), 1.0), ((('shriveled', 'JJ'), ('corpse', 'NN')), 1.0), ((('bizarre', 'JJ'), ('twist', 'NN')), 1.0), ((('exotic', 'JJ'), ('expedition', 'NN')), 1.0), ((('flash', 'JJ'), ('floods', 'NNS')), 1.0), ((('cryptic', 'JJ'), ('fish', 'NN')), 1.0), ((('added', 'JJ'), ('safety', 'NN')), 1.0), ((('asteroid', 'JJ'), ('belt', 'NN')), 1.0), ((('long-term', 'JJ'), ('climate', 'NN')), 1.0), ((('vivid', 'JJ'), ('greens', 'NNS')), 1.0), ((('enthralled', 'JJ'), ('ant', 'NN')), 1.0), ((('final', 'JJ'), ('stage', 'NN')), 1.0)]\n",
      "\n",
      "20 bigrammi Aggettivo-Sostantivo nel corpus con massima MI e relativo valore di MI: [((('fungus-caterpillar', 'JJ'), ('husk', 'NN')), 12.75), ((('nuclear', 'JJ'), ('furnaces', 'NNS')), 12.75), ((('scuba', 'JJ'), ('blog', 'NN')), 12.75), ((('evolutionary', 'JJ'), ('transformations', 'NNS')), 12.75), ((('night-dive', 'JJ'), ('specialty', 'NN')), 12.75), ((('bizarre', 'JJ'), ('twist', 'NN')), 12.75), ((('exotic', 'JJ'), ('expedition', 'NN')), 12.75), ((('asteroid', 'JJ'), ('belt', 'NN')), 12.75), ((('vivid', 'JJ'), ('greens', 'NNS')), 12.75), ((('final', 'JJ'), ('stage', 'NN')), 12.75), ((('immune', 'JJ'), ('booster', 'NN')), 12.75), ((('safe', 'JJ'), ('environment', 'NN')), 12.75), ((('humid', 'JJ'), ('microclimate', 'NN')), 12.75), ((('wrong', 'JJ'), ('direction', 'NN')), 12.75), ((('bioactive', 'JJ'), ('compounds', 'NNS')), 12.75), ((('multi-layered', 'JJ'), ('plastic', 'NN')), 12.75), ((('deadly', 'JJ'), ('parasite', 'NN')), 12.75), ((('obvious', 'JJ'), ('choice', 'NN')), 12.75), ((('slow', 'JJ'), ('release', 'NN')), 12.75), ((('Chinese', 'JJ'), ('medicine', 'NN')), 12.75)]\n"
     ]
    }
   ],
   "source": [
    "# 2.2. FUNZIONI CHE, ESTRATTI I BIGRAMMI AGGETTIVO-SOSTANTIVO...\n",
    "\n",
    "def bigrammi_aggsost(lista_token_pos):\n",
    "  bigrammi = list(nltk.bigrams(lista_token_pos)) # estraggo tutti i bigrammi del corpus dalla lista di coppie (token, PoS)\n",
    "  bigrammi_aggsost = [] # lista per i soli bigrammi aggettivo-sostantivo\n",
    "  for bigramma in bigrammi: # per ogni bigramma nella lista dei bigrammi\n",
    "    if bigramma[0][1] == \"JJ\" and (bigramma[1][1] == \"NN\" or bigramma[1][1] == \"NNS\" or bigramma[1][1] == \"NNP\" or bigramma[1][1] == \"NNPS\" ):\n",
    "      # ogni bigramma è costituito da due coppie (coppia di indice 0 e coppia di indice 1), dove ogni coppia contiene un token (= elemento di indice 0) e la rispettiva PoS (= 1)\n",
    "      # ... quindi se la PoS del primo token del bigramma è AGG e quella del secondo è SOST\n",
    "      bigrammi_aggsost.append(bigramma) # aggiungo il bigramma alla lista\n",
    "  return bigrammi_aggsost\n",
    "\n",
    "# ... RESTITUISCONO:\n",
    "\n",
    "# A) i 20 più frequenti, con relativa frequenza:\n",
    "\n",
    "def venti_bigr_aggsost_piùfreq(bigrammi):\n",
    "  frequenze = nltk.FreqDist(bigrammi)\n",
    "  return frequenze.most_common(20) \n",
    "\n",
    "bigrammi_aggsost_1 = bigrammi_aggsost(token_pos_1)\n",
    "print(f\"20 bigrammi Aggettivo-Sostantivo più frequenti nel corpus: {venti_bigr_aggsost_piùfreq(bigrammi_aggsost_1)}\")\n",
    "print()\n",
    "\n",
    "# B) i 20 con probabilità condizionata massima e relativo valore di probabilità:\n",
    "\n",
    "def venti_bigr_aggsost_max_probcond(tokens, bigrammi_aggsost):\n",
    "  vocab_bigrammi = list(set(bigrammi_aggsost)) # vocabolario dei bigrammi agg-sost (= lista senza ripetizioni)\n",
    "  lista_bigrammi_prob = [] # lista per le coppie (bigramma, probabilità)\n",
    "  for bigramma in vocab_bigrammi: # per ogni bigramma agg-sost\n",
    "    freq_bigramma = vocab_bigrammi.count(bigramma) # calcolo la frequenza\n",
    "    freq_token_1 = tokens.count(bigramma[0][0]) # calcolo la frequenza del primo token\n",
    "    prob_cond = freq_bigramma/freq_token_1 # calcolo la probabilità condizionata\n",
    "    lista_bigrammi_prob.append((bigramma, prob_cond)) # lo inserisco nella lista\n",
    "  bigrammi_ordinati = sorted(lista_bigrammi_prob, key = lambda x: x[1], reverse = True) # lista di bigrammi ordinati in base al secondo elemento ([1] = valore della probabilità) della tupla (\"key\"), e per valori decrescenti (\"reverse\")\n",
    "  return bigrammi_ordinati[:20] # a partire dalla lista, restituisco con lo slicing i primi 20 bigrammi\n",
    "\n",
    "print(f\"20 bigrammi Aggettivo-Sostantivo nel corpus con massima probabilità condizionata e relativo valore di probabilità: {venti_bigr_aggsost_max_probcond(tokens_1, bigrammi_aggsost_1)}\")\n",
    "print()\n",
    "\n",
    "# C) I 20 con forza associativa (Pointwise Mutual Information, PMI) massima, e relativa PMI:\n",
    "\n",
    "def venti_bigr_aggsost_max_mutua_info(tokens, bigrammi_aggsost):\n",
    "  vocab_bigrammi = list(set(bigrammi_aggsost))\n",
    "  bigrammi_mi = [] # lista per le coppie (bigramma, MI)\n",
    "  for bigramma in vocab_bigrammi:\n",
    "    prob_bigramma = vocab_bigrammi.count(bigramma)/len(tokens) # probabilità del bigramma = frequenza bigramma / lunghezza corpus\n",
    "    prob_token_1 = tokens.count(bigramma[0][0])/len(tokens) # probabilità del primo token del bigramma = frequenza token / lunghezza corpus\n",
    "    prob_token_2 = tokens.count(bigramma[1][0])/len(tokens) # probabilità del secondo token del bigramma = frequenza token / lunghezza corpus\n",
    "    MI = round(math.log(prob_bigramma/(prob_token_1*prob_token_2), 2), 3) # MI arrotondata a 3 cifre decimali\n",
    "    bigrammi_mi.append((bigramma, MI))\n",
    "  bigrammi_ordinati = sorted(bigrammi_mi, key = lambda x: x[1], reverse = True) \n",
    "  return bigrammi_ordinati[:20]\n",
    "        \n",
    "print(f\"20 bigrammi Aggettivo-Sostantivo nel corpus con massima MI e relativo valore di MI: {venti_bigr_aggsost_max_mutua_info(tokens_1, bigrammi_aggsost_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4sP25uFJTG2",
    "outputId": "8656147b-0d3d-40b0-be7d-8652d48b65fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase con la media della distribuzione di frequenza dei token più alta e relativa distr. di frequenza: ('Because that\\'s the cost and that\\'s the tragedy.\"', 122.667)\n",
      "\n",
      "Frase con la media della distribuzione di frequenza dei token più bassa e relativa distr. di frequenza: (\"Rain falling on snow caused this week's floods, events that will become more likely as temperatures warm.\", 0.05)\n",
      "\n",
      "Frase con probabilità più alta secondo un modello di Markov di ordine 2 costruito a partire dal corpus di input: ('Scientists expect more spring rain, less winter snow.', 0.000145)\n"
     ]
    }
   ],
   "source": [
    "# 2.3. FUNZIONI CHE, CONSIDERATE LE FRASI CON UNA LUNGHEZZA COMPRESA TRA 10 E 20 TOKEN, IN CUI ALMENO LA META' DEI TOKEN OCCORRE ALMENO 2 VOLTE NEL CORPUS...\n",
    "\n",
    "def frasi_tra10e20_token(tokens, frasi):\n",
    "  frasi_10_20 = [] # lista di frasi la cui lunghezza in token è compresa tra 10 e 20\n",
    "  for frase in frasi: # per ogni frase nella lista di tutte le frasi del corpus\n",
    "    tokens_frase = nltk.tokenize.word_tokenize(frase) # la tokenizzo\n",
    "    lunghezza_frase = len(tokens_frase) # e ne salvo la lunghezza\n",
    "    if lunghezza_frase >= 10 and lunghezza_frase <= 20: # se la lunghezza della frase è compresa tra i 10 e i 20 token\n",
    "      frasi_10_20.append(frase) # la inserisco nella lista\n",
    "  for frase in frasi_10_20: # per ogni frase lunga dai 10 ai 20 token\n",
    "    tokens_frase_10_20 = nltk.tokenize.word_tokenize(frase) # la tokenizzo\n",
    "    metà = len(tokens_frase_10_20)/2 # calcolo la metà della lunghezza della frase\n",
    "    conta_non_hapax = 0 # conto gli elementi che ricorrono più di una volta nell'intero corpus\n",
    "    for token in tokens_frase_10_20: # per ogni token della frase lunga dai 10 ai 20 token\n",
    "      if tokens.count(token) > 1: # se il token ricorre nel corpus più di una volta\n",
    "        conta_non_hapax += 1 # aggiorno il contatore\n",
    "    if conta_non_hapax < metà: # se il valore del contatore è minore del valore della metà dei token della frase\n",
    "      frasi_10_20.remove(frase) # rimuovo la frase dalla lista\n",
    "    return frasi_10_20\n",
    "\n",
    "# ... RESTITUISCONO: \n",
    "\n",
    "# A. La frase con la media della distribuzione di frequenza dei token più alta:\n",
    "\n",
    "def media_max_distr_freq(tokens, frasi):\n",
    "  media_max = 0 # valore massimo (provvisorio) della media\n",
    "  for frase in frasi: # per ogni frase nella lista di tutte le frasi del corpus\n",
    "    tokens_frase = nltk.tokenize.word_tokenize(frase) # la tokenizzo\n",
    "    lunghezza = len(tokens_frase) # ne salvo la lunghezza\n",
    "    frequenza = 0 # contatore per la frequenza (nel corpus) dei token della frase\n",
    "    for token in tokens_frase: # per ogni token di quelli della frase\n",
    "      frequenza += tokens.count(token) # calcolo la somma di tutte le volte in cui ricorre nel corpus \n",
    "      media = round(frequenza/lunghezza, 3) # calcolo la media della distribuzione di frequenza del token (arrotondata a 3 cifre decimali)\n",
    "      if (media > media_max): # se il valore della media appena calcolato è maggiore di quello massimo al momento\n",
    "        media_max = media # quello appena calcolato è il nuovo valore massimo e quindi lo sotituisco  \n",
    "        frase_max = frase # memorizzo in una variabile la frase che ha quel valore\n",
    "  return frase_max, media_max\n",
    "\n",
    "print(f\"Frase con la media della distribuzione di frequenza dei token più alta e relativa distr. di frequenza: {media_max_distr_freq(tokens_1, (frasi_tra10e20_token(tokens_1, frasi_1)))}\")\n",
    "print()\n",
    "\n",
    "# B. La frase con la media della distribuzione di frequenza dei token più bassa:\n",
    "\n",
    "def media_min_distr_freq(tokens, frasi):\n",
    "  media_min = media_max_distr_freq(tokens, frasi_tra10e20_token(tokens, frasi))[1] # valore minimo (provvisorio) della media; in questo caso, è il valore massimo della media (\"media_max\") calcolato con la funzione precedente\n",
    "  for frase in frasi: \n",
    "    tokens_frase = nltk.tokenize.word_tokenize(frase) \n",
    "    lunghezza = len(tokens_frase) \n",
    "    frequenza = 0 \n",
    "    for token in tokens_frase:\n",
    "      frequenza += tokens.count(token) \n",
    "      media = round(frequenza/lunghezza, 3) \n",
    "      if (media < media_min): # se il valore della media appena calcolato è minore di quello minimo al momento\n",
    "        media_min = media # quello appena calcolato è il nuovo valore minimo e quindi lo sotituisco  \n",
    "        frase_min = frase \n",
    "  return frase_min, media_min\n",
    "\n",
    "print(f\"Frase con la media della distribuzione di frequenza dei token più bassa e relativa distr. di frequenza: {media_min_distr_freq(tokens_1, (frasi_tra10e20_token(tokens_1, frasi_1)))}\")\n",
    "print()\n",
    "\n",
    "# C. La frase con probabilità più alta secondo un modello di Markov di ordine 2 costruito a partire dal corpus di input:\n",
    "\n",
    "def prob_cond_markov2(trigramma, bigramma, tokens): # funzione che restituisce la probabilità condizionata del trigramma dato il bigramma (trattandosi di un modello markoviano di ordine 2)\n",
    "  bigrammi = list(nltk.bigrams(tokens)) # lista dei bigrammi del corpus\n",
    "  trigrammi = list(nltk.trigrams(tokens)) # lista dei trigrammi del corpus\n",
    "  prob_cond = trigrammi.count(trigramma) / bigrammi.count(bigramma) # probabilità condizionata = frequenza del trigramma / frequenza del bigramma\n",
    "  return prob_cond\n",
    "\n",
    "def max_prob_markov_2(tokens, frasi): \n",
    "  bigrammi = list(nltk.bigrams(tokens)) # lista dei bigrammi del corpus\n",
    "  prob_max = 0 # valore massimo (provvisorio) della probabilità\n",
    "  for frase in frasi: # per ogni frase nella lista di tutte le frasi del corpus\n",
    "    tokens_frase = nltk.tokenize.word_tokenize(frase) # la tokenizzo\n",
    "    trigrammi_frase = list(nltk.trigrams(tokens_frase)) # lista per i trigrammi della frase\n",
    "    bigrammi_frase = list(nltk.bigrams(tokens_frase)) # lista per i bigrammi della frase\n",
    "    # probabilità del primo elemento della catena markoviana = probabilità (frequenza relativa) del primo token della frase: \n",
    "    prob_1 = tokens.count(tokens_frase[0]) / len(tokens) # frequenza del token nel corpus / lunghezza del corpus\n",
    "    # probabilità del secondo elemento della catena markoviana = probabilità (frequenza relativa) del primo bigramma dato il primo token:\n",
    "    prob_2 = bigrammi.count(bigrammi_frase[0]) / tokens.count(bigrammi_frase[0][0]) # frequenza del primo bigramma della frase nella lista dei bigrammi del corpus / frequenza del primo token del bigramma nel corpus\n",
    "    prob_frase = round((prob_1*prob_2), 6) # probabilità provvisoria della frase (arrotondata a 6 cifre decimali)\n",
    "    # probabilità degli altri elementi della catena markoviana = prob. dei trigrammi dati i bigrammi\n",
    "    for trigramma in trigrammi_frase: # per ogni trigramma della frase\n",
    "      for bigramma in bigrammi_frase: # per ogni bigramma della frase\n",
    "        prob_cong = prob_cond_markov2(trigramma, bigramma, tokens) # calcolo la probabilità condizionata del trigramma dato il rispettivo bigramma\n",
    "        prob_frase *= prob_cong # aggiorno la probabilità calcolata sopra moltiplicando al suo valore, di volta in volta, i vari prodotti delle probabilità dei trigrammi\n",
    "    if prob_frase > prob_max:\n",
    "      prob_max = prob_frase\n",
    "      frase_max = frase\n",
    "  return frase_max, prob_max\n",
    "\n",
    "print(f\"Frase con probabilità più alta secondo un modello di Markov di ordine 2 costruito a partire dal corpus di input: {max_prob_markov_2(tokens_1, (frasi_tra10e20_token(tokens_1, frasi_1)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRuZkfrnJ3ae",
    "outputId": "3b26e275-eb44-4a56-b164-31700b42cf86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 NE 'PERSON' più frequenti nel corpus: [('Earth', 7), ('Yellowstone National Park', 7), ('Cardman', 7), ('Bennett', 5), ('Rodman', 5), ('Yellowstone', 4), ('Feustel', 4), ('Bekker', 4), ('Artemis', 3), ('Love', 3), ('Mars', 2), ('Becker', 2), ('Keck', 2), ('Roads', 2), ('Stein', 2)]\n",
      "\n",
      "15 NE 'GPE' più frequenti nel corpus: [('Earth', 3), ('Yellowstone', 3), ('United States', 2), ('Arizona', 2), ('Japanese', 2), ('Thailand', 2), ('Astrophysics', 1), ('Hawaii', 1), ('Jovian', 1), ('June', 1), ('Montana', 1), ('Gardiner', 1), ('Alaska', 1), ('Long', 1), ('Houston', 1)]\n",
      "\n",
      "15 NE 'ORGANIZATION' più frequenti nel corpus: [('NASA', 19), ('JETT3', 7), ('University', 3), ('PADI', 3), ('Yellowstone River', 2), ('SP Crater', 2), ('EVA', 2), ('Maldives', 2), ('Ohio', 1), ('Mercury', 1), ('Microlensing Observations', 1), ('TESS', 1), ('Greater Yellowstone Climate Assessment', 1), ('Geographic Information Systems', 1), ('Memorial', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 2.4. FUNZIONE CHE, ESTRATTE LE ENTITA' NOMINATE (NE) DEL TESTO, RESTITUISCE PER CIASCUNA CLASSE DI NE I 15 ELEMENTI PIU' FREQUENTI, ORDINATI PER FREQUENZA DISCENDENTE E CON RELATIVA FREQUENZA:\n",
    "\n",
    "def quindici_ne_più_freq(token_pos, classe_NE):\n",
    "  albero_NE = nltk.ne_chunk(token_pos) # albero di tutte le NE nel testo\n",
    "  token_NE = [] # lista per le coppie (token, classe di NE)\n",
    "  lista_classe_NE = [] # lista per i soli elementi appartenenti ad una certa classe di NE (PERSON o GPE o ORGANIZATION)\n",
    "  for nodo in albero_NE: # per ogni nodo dell'albero\n",
    "    if hasattr(nodo, \"label\"): # se il nodo ha un attributo di tipo \"label\"\n",
    "      tipo_entità = nodo.label() # estraggo la classe di NE\n",
    "      token_entità = \" \".join([token for token, pos in nodo.leaves()]) # scorrendo le foglie dell'albero, considero solo i token\n",
    "      token_NE.append((token_entità, tipo_entità))\n",
    "  for token_entità, tipo_entità in token_NE: # per ogni coppia (token, classe di NE) nella lista\n",
    "    if tipo_entità == classe_NE: # se la classe di NE è uguale a quella in input\n",
    "      lista_classe_NE.append(token_entità) # inserisco il token nella lista apposita per quello specifico tag\n",
    "  frequenze = nltk.FreqDist(lista_classe_NE) # calcolo le frequenze di ciascuna delle 3 liste con FreqDist\n",
    "  return frequenze.most_common(15) #si restituiscono i 15 elementi più frequenti\n",
    "\n",
    "print(f\"15 NE 'PERSON' più frequenti nel corpus: {quindici_ne_più_freq(token_pos_1, 'PERSON')}\")\n",
    "print()\n",
    "print(f\"15 NE 'GPE' più frequenti nel corpus: {quindici_ne_più_freq(token_pos_1, 'GPE')}\")\n",
    "print()\n",
    "print(f\"15 NE 'ORGANIZATION' più frequenti nel corpus: {quindici_ne_più_freq(token_pos_1, 'ORGANIZATION')}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
